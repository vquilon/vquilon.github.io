---
title: Unraveling LLM Weights Formats, A Comprehensive Guide
tags: [React, JavaScript, Web Development]
author: V√≠ctor Quil√≥n Ranera
style: border
color: primary
description: Explore the diverse formats of LLM model weights, their structures, and how they impact performance and compatibility in machine learning applications.
---

Existen diversos formatos de archivo para almacenar los pesos de modelos de lenguaje de gran tama√±o (LLM), cada uno con su propia historia, creadores y frameworks asociados. A continuaci√≥n, se detallan los principales formatos, sus or√≠genes y las implicaciones de su uso en diferentes arquitecturas.

---

## üß† Formatos de archivo para pesos de LLM: historia, creadores y frameworks

### 1. **PyTorch `.pt` / `.bin`**

* **Origen**: Formato nativo de PyTorch, ampliamente utilizado para guardar modelos en su forma original.
* **Creadores**: Desarrollado por los creadores de PyTorch en Facebook AI Research.
* **Framework asociado**: PyTorch.
* **Caracter√≠sticas**:

  * Almacena modelos en precisi√≥n completa (float32 o float16).
  * Utilizado principalmente para entrenamiento y fine-tuning.
  * No est√° optimizado para inferencia eficiente en dispositivos con recursos limitados.([arxiv.org][1])

### 2. **SafeTensors `.safetensors`**

* **Origen**: Introducido como una alternativa segura y eficiente a los archivos `.pt` de PyTorch.
* **Creadores**: Desarrollado por Hugging Face.
* **Framework asociado**: Compatible con PyTorch y otros frameworks.
* **Caracter√≠sticas**:

  * Formato binario seguro que evita la ejecuci√≥n de c√≥digo arbitrario al cargar modelos.
  * M√°s r√°pido y seguro que los archivos `.pt`.
  * Soporta m√∫ltiples tipos de datos y es adecuado para compartir modelos de manera segura.

### 3. **GGUF (GGML Universal File)**

* **Origen**: Introducido en agosto de 2023 por el proyecto llama.cpp.
* **Creadores**: Georgi Gerganov y colaboradores del proyecto llama.cpp.
* **Framework asociado**: llama.cpp y GGML.
* **Caracter√≠sticas**:

  * Formato binario que almacena tensores y metadatos en un solo archivo.
  * Dise√±ado para una carga y almacenamiento r√°pidos de datos de modelos.
  * Soporta m√∫ltiples tipos de cuantizaci√≥n (de 2 a 8 bits) y formatos de punto flotante (float32, float16, bfloat16).
  * Incluye informaci√≥n necesaria para ejecutar modelos tipo GPT, como vocabulario del tokenizador, longitud del contexto e informaci√≥n de tensores.
  * Compatible con m√∫ltiples arquitecturas de modelos, incluyendo LLaMA, Mistral, BERT, GPT-2, entre otros. ([en.wikipedia.org][2], [en.wikipedia.org][3])

### 4. **EXL2**

* **Origen**: Desarrollado como una evoluci√≥n de formatos anteriores como GPTQ y AWQ, centrado en la eficiencia de inferencia.
* **Creadores**: Comunidad de desarrolladores de modelos locales, como LoneStriker y TheBloke.
* **Framework asociado**: Utilizado con herramientas como exllamav2.
* **Caracter√≠sticas**:

  * Optimizado para inferencia en GPU, especialmente en configuraciones multi-GPU.
  * Ofrece velocidades de inferencia superiores en comparaci√≥n con otros formatos, alcanzando hasta 15-20 tokens por segundo en modelos de 70B par√°metros.
  * Soporta diferentes niveles de cuantizaci√≥n, como 2.4bpw (bits por peso) hasta 6.0bpw.
  * Requiere menos VRAM en comparaci√≥n con otros formatos para modelos del mismo tama√±o. ([reddit.com][4], [reddit.com][5])

---

## ‚öôÔ∏è Rendimiento en diferentes arquitecturas y sus implicaciones

El rendimiento de los modelos LLM puede variar significativamente seg√∫n el formato de archivo utilizado y la arquitectura del hardware en el que se ejecutan.

### **CPU vs. GPU**

* **GGUF**:

  * Dise√±ado para ser eficiente en CPU, aprovechando extensiones como AVX, AVX2, AVX-512 en x86 y Neon en ARM.
  * Soporta m√∫ltiples backends, incluyendo CUDA, Metal, Vulkan y SYCL, lo que permite su ejecuci√≥n en diversas plataformas. ([en.wikipedia.org][2])

* **EXL2**:

  * Optimizado para ejecuci√≥n en GPU, especialmente en configuraciones multi-GPU.
  * Ofrece velocidades de inferencia superiores en comparaci√≥n con GGUF en entornos con m√∫ltiples GPUs. ([reddit.com][4])

### **Uso de memoria y velocidad de inferencia**

* **GGUF**:

  * Ofrece un equilibrio entre uso de memoria y velocidad de inferencia, siendo adecuado para dispositivos con recursos limitados.
  * La velocidad de inferencia puede ser menor en comparaci√≥n con EXL2 en configuraciones de GPU de alto rendimiento.([reddit.com][4])

* **EXL2**:

  * Requiere menos VRAM para modelos grandes, permitiendo la ejecuci√≥n de modelos de hasta 70B par√°metros en GPUs con 24 GB de VRAM.
  * Ofrece velocidades de inferencia m√°s altas, especialmente en configuraciones multi-GPU.&#x20;

---

## üß© Implicaciones pr√°cticas

* **Compatibilidad**: La elecci√≥n del formato puede afectar la compatibilidad con diferentes herramientas y frameworks. Por ejemplo, GGUF es ampliamente compatible con llama.cpp, mientras que EXL2 se utiliza con herramientas espec√≠ficas como exllamav2.([reddit.com][5])

* **Facilidad de uso**: Formatos como SafeTensors ofrecen ventajas en t√©rminos de seguridad y facilidad de uso, evitando la ejecuci√≥n de c√≥digo arbitrario al cargar modelos.([juicefs.medium.com][6])

* **Desempe√±o**: La elecci√≥n del formato puede tener un impacto significativo en el desempe√±o del modelo, especialmente en t√©rminos de velocidad de inferencia y uso de memoria.

---

En resumen, la elecci√≥n del formato de archivo para almacenar los pesos de modelos LLM depende de m√∫ltiples factores, incluyendo el entorno de ejecuci√≥n, los recursos disponibles y las necesidades espec√≠ficas de rendimiento y compatibilidad.

---

## üß© ONNX (Open Neural Network Exchange)

### üìú Historia y creadores

ONNX fue creado en 2017 por Facebook y Microsoft como un formato abierto para representar modelos de aprendizaje autom√°tico. Su objetivo principal es facilitar la interoperabilidad entre diferentes frameworks de IA, permitiendo a los desarrolladores mover modelos entre herramientas como PyTorch, TensorFlow, Caffe2 y MXNet sin necesidad de reentrenarlos .([en.wikipedia.org][1], [aijobs.net][2])

### üîß Frameworks asociados

ONNX es compatible con una amplia gama de frameworks y herramientas de IA, incluyendo PyTorch, TensorFlow, Caffe2, MXNet y OpenCV. Adem√°s, cuenta con su propio motor de inferencia, ONNX Runtime, que permite ejecutar modelos en diversas plataformas y dispositivos .([de.wikipedia.org][3], [youtube.com][4])

### ‚öôÔ∏è Rendimiento en diferentes arquitecturas

* **CPU y GPU**: ONNX Runtime est√° optimizado para ejecutarse tanto en CPU como en GPU, aprovechando las capacidades de hardware para mejorar el rendimiento.

* **Limitaciones con LLMs**: Aunque ONNX es eficaz para una amplia variedad de modelos, ha enfrentado desaf√≠os al trabajar con LLMs debido a su complejidad y tama√±o. Algunos usuarios han se√±alado que ONNX no ha logrado el mismo nivel de √©xito en el √°mbito de los LLMs en comparaci√≥n con formatos espec√≠ficos como GGUF .([reddit.com][5])

---

## üîß MLC (Machine Learning Compilation)

### üìú Historia y creadores

MLC LLM es un compilador de aprendizaje autom√°tico y motor de despliegue de alto rendimiento para modelos de lenguaje de gran tama√±o. Su misi√≥n es permitir que cualquier persona desarrolle, optimice y despliegue modelos de IA de forma nativa en diversas plataformas .([github.com][6])

### üîß Frameworks asociados

MLC LLM se integra con MLCEngine, un motor de inferencia de alto rendimiento compatible con m√∫ltiples plataformas, incluyendo REST, Python, JavaScript, iOS y Android. Esto permite una implementaci√≥n flexible y eficiente de modelos LLM en una variedad de dispositivos y entornos .([llm.mlc.ai][7])

### ‚öôÔ∏è Rendimiento en diferentes arquitecturas

* **Dispositivos m√≥viles y edge**: MLC est√° dise√±ado para optimizar la ejecuci√≥n de modelos LLM en dispositivos con recursos limitados, como smartphones y dispositivos edge, mediante t√©cnicas de compilaci√≥n que reducen el uso de memoria y mejoran la velocidad de inferencia .([cmu.edu][8])

* **Compatibilidad multiplataforma**: Gracias a su enfoque en la compilaci√≥n y optimizaci√≥n, MLC permite ejecutar modelos LLM de forma eficiente en una amplia gama de plataformas, desde servidores hasta dispositivos m√≥viles.

---

## üß† Comparativa y consideraciones

| Caracter√≠stica          | ONNX                                             | MLC LLM                                             |
| ----------------------- | ------------------------------------------------ | --------------------------------------------------- |
| **Creadores**           | Facebook y Microsoft                             | Comunidad de MLC LLM                                |
| **A√±o de creaci√≥n**     | 2017                                             | N/D                                                 |
| **Enfoque principal**   | Interoperabilidad entre frameworks               | Compilaci√≥n y despliegue eficiente de LLMs          |
| **Compatibilidad**      | Amplia (PyTorch, TensorFlow, etc.)               | Multiplataforma (REST, Python, iOS, Android)        |
| **Rendimiento en LLMs** | Limitado en comparaci√≥n con formatos espec√≠ficos | Optimizado para dispositivos con recursos limitados |
| **Uso de memoria**      | Variable seg√∫n el modelo y la plataforma         | Eficiente mediante t√©cnicas de compilaci√≥n          |

---

En resumen, **ONNX** es una soluci√≥n robusta para la interoperabilidad de modelos de aprendizaje autom√°tico entre diferentes frameworks, pero puede enfrentar limitaciones al trabajar con LLMs debido a su tama√±o y complejidad. Por otro lado, **MLC LLM** ofrece una alternativa enfocada en la compilaci√≥n y despliegue eficiente de modelos LLM en una variedad de plataformas, incluyendo dispositivos con recursos limitados. La elecci√≥n entre ambos depender√° de las necesidades espec√≠ficas del proyecto, considerando factores como la plataforma de destino, los recursos disponibles y los requisitos de rendimiento.

[1]: https://arxiv.org/abs/2106.09685?utm_source=chatgpt.com "LoRA: Low-Rank Adaptation of Large Language Models"
[2]: https://en.wikipedia.org/wiki/Llama.cpp?utm_source=chatgpt.com "Llama.cpp"
[3]: https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com "Llama (language model)"
[4]: https://www.reddit.com/r/LocalLLaMA/comments/1ayd4xr?utm_source=chatgpt.com "For those who don't know what different model formats (GGUF, GPTQ, AWQ, EXL2, etc.) mean ‚Üì"
[5]: https://www.reddit.com/r/LocalLLaMA/comments/17w57eu?utm_source=chatgpt.com "üê∫üê¶‚Äç‚¨õ LLM Format Comparison/Benchmark: 70B GGUF vs. EXL2 (and AWQ)"
[6]: https://juicefs.medium.com/llm-storage-performance-cost-and-multi-cloud-architecture-96be5e495dc8?utm_source=chatgpt.com "LLM Storage: Performance, Cost, and Multi-Cloud Architecture | by JuiceFS | Medium"


[1]: https://en.wikipedia.org/wiki/Open_Neural_Network_Exchange?utm_source=chatgpt.com "Open Neural Network Exchange"
[2]: https://aijobs.net/insights/onnx-explained/?utm_source=chatgpt.com "ONNX explained - aijobs.net"
[3]: https://de.wikipedia.org/wiki/ONNX?utm_source=chatgpt.com "ONNX"
[4]: https://www.youtube.com/watch?v=jrIJT01E8Xw&utm_source=chatgpt.com "Large Language Model inference with ONNX Runtime (Kunal ..."
[5]: https://www.reddit.com/r/LocalLLaMA/comments/1h54n1u/why_didnt_onnx_succeed_in_the_llm_world/?utm_source=chatgpt.com "Why didn't ONNX succeed in the LLM world? : r/LocalLLaMA - Reddit"
[6]: https://github.com/mlc-ai/mlc-llm?utm_source=chatgpt.com "mlc-ai/mlc-llm: Universal LLM Deployment Engine with ML ... - GitHub"
[7]: https://llm.mlc.ai/?utm_source=chatgpt.com "MLC LLM | Home"
[8]: https://www.cmu.edu/flame/research/2024/machine-learning-compilation-for-large-language-models.html?utm_source=chatgpt.com "Machine Learning Compilation for Large Language Models (MLC ..."
